{
	"codemonkey": {
		"providers": [
			{
			"name": "OpenRouter",
			"enabled": false,
			"baseUrl": "https://openrouter.ai/api/v1",
			"apiKey": "YOUR_OPENROUTER_API_KEY_HERE",
			"models": ["openai/gpt-4o", "anthropic/claude-3.5-sonnet", "anthropic/claude-3-haiku"],
			"description": "Access multiple AI models through one API"
		},
			{
				"name": "OpenAI",
				"enabled": false,
				"baseUrl": "https://api.openai.com/v1",
				"apiKey": "YOUR_OPENAI_API_KEY_HERE",
				"models": ["gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo"],
				"description": "Direct OpenAI API access"
			},
			{
				"name": "Anthropic",
				"enabled": false,
				"baseUrl": "https://api.anthropic.com/v1",
				"apiKey": "YOUR_ANTHROPIC_API_KEY_HERE",
				"models": ["claude-3-5-sonnet-20241022", "claude-3-opus-20240229", "claude-3-haiku-20240307"],
				"description": "Direct Anthropic Claude API access"
			},
			{
				"name": "Local Ollama",
				"enabled": true,
				"baseUrl": "http://localhost:11434/v1",
				"models": [],
				"autoDetectModels": true,
				"description": "Run models locally (free, private, no API key needed) - Auto-detects installed models"
			},
			{
				"name": "Hugging Face",
				"enabled": false,
				"baseUrl": "https://api-inference.huggingface.co/v1",
				"apiKey": "YOUR_HUGGINGFACE_API_KEY_HERE",
				"models": [
					"meta-llama/Meta-Llama-3.1-70B-Instruct",
					"Qwen/Qwen2.5-Coder-32B-Instruct",
					"mistralai/Mixtral-8x7B-Instruct-v0.1",
					"microsoft/Phi-3-medium-4k-instruct"
				],
				"description": "Access 1000s of models via Hugging Face Inference API"
			},
			{
				"name": "Groq",
				"enabled": false,
				"baseUrl": "https://api.groq.com/openai/v1",
				"apiKey": "YOUR_GROQ_API_KEY_HERE",
				"models": ["llama-3.1-70b-versatile", "mixtral-8x7b-32768"],
				"description": "Ultra-fast inference with Groq LPU"
			},
			{
				"name": "OpenAI Compatible",
				"enabled": false,
				"baseUrl": "http://localhost:1234/v1",
				"apiKey": "optional-api-key-if-needed",
				"models": ["local-model-1", "local-model-2"],
				"description": "LM Studio, vLLM, or any OpenAI-compatible server"
			}
		],
		"mcpServers": []
	}
}
